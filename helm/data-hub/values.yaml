airflow:
  service:
    type: "LoadBalancer"
    ## dns hostname if when using external dns for the load balancer service
    #dnsHostName: data-pipeline.my-organisation.org
    #externalPort: 443
    ## Client certificate for https service
    #clientCertificate: "arn:aws:iam::32848334893:server-certificate/cloudfront/wildcard.my-organisation.org/2019.wildcard.my-organisation.org"
  image:
    ## docker-airflow image with dask executor
    repository: elifesciences/data-hub-with-dags_unstable
    #repository: elifesciences/data-hub-airflow
    ## image tag
    tag: 1575b089d529be3ff7247de3aeaa6f9ca3f6f53a
    ## Image pull policy
    ## values: Always or IfNotPresent
    pullPolicy: IfNotPresent
    ## image pull secret for private images
    pullSecret:
  ## Set schedulerNumRuns to control how the scheduler behaves:
  ##   -1 will let him looping indefinitively but it will never update the DAG
  ##   1 will have the scheduler quit after each refresh, but kubernetes will restart it.
  ## A long running scheduler process, ends up not scheduling some tasks. We still donâ€™t know the exact cause,
  ## unfortunately. Airflow has a built-in workaround in the form of the `num_runs` flag.
  schedulerNumRuns: "-1"
  ## Number of replicas for web server.
  webReplicas: 1
  ## Custom  environment variables
  ## Use this to add environment variables to the container (including airflow variables to be overriden)
  customEnvironmentVariables:
   # DAG_APP_CONFIGURATION - Directory in the pod where data pipeline application configuration files are
   # mounted.
    DAG_APP_CONFIGURATION: /home/airflow/app-config
    EXTRACT_KEYWORDS_SCHEDULE_INTERVAL: "@daily"
    GOOGLE_SPREADSHEET_SCHEDULE_INTERVAL: "@daily"
    CROSS_REF_IMPORT_SCHEDULE_INTERVAL: "@daily"
    CROSSREF_CONFIG_FILE_PATH: "/home/airflow/app-config/crossref-event/crossref-event-data-pipeline.config.yaml"
    SPREADSHEET_CONFIG_FILE_PATH: "/home/airflow/app-config/google-spreadsheet/spreadsheet-data-pipeline.config.yaml"
    GOOGLE_APPLICATION_CREDENTIALS: /home/airflow/.gcp/gcp_credentials.json
    #provides list of email addresses to which the airflow can send emails when required to
    AIRFLOW_NOTIFICATION_EMAIL_CSV_LIST: ""


  ## Secrets which will be mounted as a file at `secretsDir/<secret name>`.
  ## They should be created before helm is deployed
  #secrets:
  #  - name: credentials #name of the secret
  #    secretsDir: /home/airflow/.aws # where they are mounted in the pod
  #  - name: gcp-credentials
  #    secretsDir: /home/airflow/.gcp
  ## Configure pod disruption budget for the scheduler
  podDisruptionBudget:
    maxUnavailable: 1

  ## Run initdb when the scheduler starts.
  initdb: true

web:
  # K8s resource spec for webserver pod
  strategy:
    # Smooth rolling update of the Web UI
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 100%
  resources:
    # limits:
    #   cpu: "300m"
    #   memory: "1Gi"
    #requests:
    #  cpu: 1
    #  memory: "1Gi"
  initialStartupDelay: "60"
  initialDelaySeconds: "360"

websiteAuthentication:
  ## Set to true if google Oauth2 should be used for authentication
  enabled: false
  # keys AIRFLOW__GOOGLE__CLIENT_ID and AIRFLOW__GOOGLE__CLIENT_SECRET  (whose values must be created from google api)
  # Must be pre-created as secret
  googleOAuthSecretName: google-auth
  googleOAuthCallbackRoute: "/oauth2callback"
  googleOAuthAuthenticatedDomain: "elifesciences.org"

scheduler:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 100%
  resources: {}
    # limits:
    #   cpu: "1000m"
    #   memory: "1Gi"
    # requests:
    #   cpu: "500m"
    #   memory: "512Mi"


dagAplicationConfigurationFiles:
  # This is used for mounting data pipeline configuration as configuration files in the pod using the
  # configmap https://github.com/elifesciences/elife-data-hub-charts/blob/develop/helm/data-hub/templates/dag-app-configuration-configmap.yaml
  # Each array entry represent a key-value pair of the configmap data entry, as well as the location where
  # that configmap entry should be mounted in the pod

  mountedFiles:
  # key: here represent the keyname of the configmap data record entry
  # - key: mount-dir/new-file-name1.extension
  # fileContent  - is the content of the data pipeline configuration, note that this file content can be
  # given directly in the values.yaml or it can be provided by overriden the values as prt of the commandline parameters
  #   fileContent: |
  #      gcpProj: some-gcp-proj
  #      dataPipelineId: some data pipelineid
  #   relativeMountedPath - is the directory where the file should be mounted relatve to the
  # airflow.customEnvironmentVariables.DAG_APP_CONFIGURATION defined about
  #   relativeMountedPath: mount-dir/new-file-name1.extension
  # - fileContent: ./some-dir/somefile2.extension
  #    relativeMountedPath:  mount-dir/new-file-name2.extension
  #    key: mount-dir/new-file-name1.extension

# configuration for the smtp service as given in
# https://github.com/elifesciences/elife-data-hub-charts/blob/develop/helm/data-hub/templates/airflow-env-configmap.yaml
airflowSmtp:
  enabled: false
  smtpServer:
    deployLocal: false
    smtpImage: namshi/smtp
  smtpServerName: smtp-server
  smtpServerPort: 25
  smtpMailFrom: do-not-reply@donotreply.com
  startTLS: false
  useSSL: false
  credential:
    enabled: false
    user: username
    password: password


## Configure logs
logs:
  # directory where the logs should be written in the pods
  path: /opt/airflow/logs

## Configure DAGs deployment and update
dags:
  ## Note that this location is referred to in airflow.cfg, so if you change it, you must update airflow.cfg accordingly.
  path: /opt/airflow/dags
  ##
  ## Set to True to prevent pickling DAGs from scheduler to workers
  doNotPickle: false


dask:
  scheduler:
    name: scheduler
    strategy:
      type: RollingUpdate
    replicas: 1
    # "NodePort", "LoadBalancer" "ClusterIP"
    serviceType: "NodePort"
    servicePort: 8786
    resources: {}
    #  limits:
    #    cpu: 1.8
    #    memory: 6G
    #  requests:
    #    cpu: 1.8
    #    memory: 6G

  # dask web ui port
  # configure the webui to monitor the dask workers
  # e.g. tasks cpu and memory usage and duration
  webUI:
    name: webui
    servicePort: 8787
  # confgures the dask worker
  worker:
    name: worker
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 100%
    replicas: 2
    noOfProcessesPerPod: 20
    # the dask memory limit only considers dask data structures
    daskMemoryLimit: ""
    resources: {}
    dask_worker: "dask-worker"
    #  limits:
    #    cpu: 1
    #    memory: 3G
    #    nvidia.com/gpu: 1
    #  requests:
    #    cpu: 1
    #    memory: 3G
    #    nvidia.com/gpu: 1

##
## Configuration values for the postgresql dependency.
postgresql:
  ##
  ## Use the PostgreSQL chart dependency.
  ## Set to false if bringing your own PostgreSQL.
  enabled: true
  ##
  ## The name of an existing secret that contains the postgres password.
  existingSecret: airflow-postgres
  ## Name of the key containing the secret.
  existingSecretKey: postgres-password
  ##
  service:
    port: 5432
  ## PostgreSQL User to create.
  postgresUser: postgres
  ##
  ## PostgreSQL Database to create.
  postgresDatabase: airflow
  ##
  ## Persistent Volume Storage configuration.
  persistence:
    ##
    ## Enable PostgreSQL persistence using Persistent Volume Claims.
    enabled: true
    ##
    ## Persistant class
    # storageClass: classname
    ##
    ## Access mode:
    accessMode: ReadWriteOnce
